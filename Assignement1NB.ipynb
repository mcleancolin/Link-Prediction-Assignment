{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv, random, json\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data into a list\n",
    "with open('train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(' ', '\\t') for line in lines]\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "\n",
    "training_file = []\n",
    "for row in d:\n",
    "    training_file.append([ int(x) for x in row ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data into a list\n",
    "test_file = pd.read_csv('test-public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the features of the nodes\n",
    "with open('nodes.json', 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the test data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for i,j in test_file.iterrows():\n",
    "    node_list_1.append(j[1])\n",
    "    node_list_2.append(j[2])\n",
    "\n",
    "test_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n",
    "edge_tuples = []\n",
    "for index, row in test_edge_list.iterrows():\n",
    "    edge_tuples.append((row['node_1'],row['node_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the training data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for row in training_file:\n",
    "    for connection in range(0,len(row)):\n",
    "        if connection == 0:\n",
    "            continue\n",
    "        else:\n",
    "            node_list_1.append(row[0])\n",
    "            node_list_2.append(row[connection])\n",
    "\n",
    "training_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53872\n",
      "43097\n",
      "43097\n",
      "10775\n",
      "10775\n"
     ]
    }
   ],
   "source": [
    "# splitting up the list of positive edges in the graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.ones(len(training_edge_list))\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_edge_list, y, test_size=0.20)\n",
    "print(len(training_edge_list))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a graph from the list of edges in the training data\n",
    "def create_graph(edge_list):\n",
    "    graph_edges = nx.from_pandas_edgelist(edge_list, \"node_1\", \"node_2\")\n",
    "    graph = nx.path_graph(4085)\n",
    "    graph.add_edges_from(graph_edges.edges())\n",
    "    n = graph.number_of_nodes()\n",
    "    m = graph.number_of_edges()\n",
    "    print(\"Number of nodes :\", str(n))\n",
    "    print(\"Number of edges :\", str(m))\n",
    "    print(\"Number of connected components :\" + str(nx.number_connected_components(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 29940\n",
      "Number of connected components :1\n",
      "-------------------------------\n",
      "Validation graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 13794\n",
      "Number of connected components :1\n"
     ]
    }
   ],
   "source": [
    "print(\"Training graph description: \")\n",
    "graph = create_graph(X_train)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "print(\"Validation graph description: \")\n",
    "validation_graph = create_graph(X_test)\n",
    "validation_graph.remove_edges_from(graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features to the nodes\n",
    "def add_features(graph):\n",
    "    for entry in features:\n",
    "        id = entry['id']\n",
    "        for key in entry:\n",
    "            graph.nodes[id][key] = entry[key]\n",
    "add_features(graph)\n",
    "add_features(validation_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_and_last(node):\n",
    "    first = node[\"first\"]  \n",
    "    last = node[\"last\"]\n",
    "    return first, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts data from the nodes such as first paper, last paper, and the difference between them\n",
    "def extract_activity_data(graph, node1, node2):\n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]\n",
    "    F1, L1 = find_first_and_last(node1)\n",
    "    F2, L2 = find_first_and_last(node2)\n",
    "    num_papers1 = node1[\"num_papers\"] \n",
    "    num_papers2 = node2[\"num_papers\"] \n",
    "    years_active_1 = (F1 - L1) + 1\n",
    "    years_active_2 = (F2 - L2) + 1\n",
    "    \n",
    "    vector = [abs(F1-F2), abs(L1-L2), num_papers1+num_papers2, years_active_1+years_active_2]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of venue and keyword entries the given node has\n",
    "def total_venues_and_keywords(node_feature_dictionary):\n",
    "    total_venues = 0\n",
    "    total_keywords = 0\n",
    "    for attribute in node_feature_dictionary:\n",
    "        if attribute[0:5] == \"venue\":\n",
    "            total_venues += 1\n",
    "        if attribute[0:7] == \"keyword\":\n",
    "            total_keywords += 1\n",
    "        \n",
    "    return total_venues, total_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of venues and keywords the two nodes have in common\n",
    "def shared_venues_and_keywords(node1, node2):\n",
    "    total_venues = 0\n",
    "    total_keywords = 0\n",
    "    for attribute in node1:\n",
    "        if attribute in node2:\n",
    "            if attribute[0:5] == \"venue\":\n",
    "                total_venues += 1\n",
    "            if attribute[0:7] == \"keyword\":\n",
    "                total_keywords += 1\n",
    "    return total_venues, total_keywords\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the number of papers the author writes per year active\n",
    "def find_efficacy(node1, node2):\n",
    "    num_papers1 = node1[\"num_papers\"] \n",
    "    num_papers2 = node2[\"num_papers\"] \n",
    "    F1, L1 = find_first_and_last(node1)\n",
    "    F2, L2 = find_first_and_last(node2)\n",
    "    years_active_1 = (F1 - L1) + 1\n",
    "    years_active_2 = (F2 - L2) + 1\n",
    "    efficacy1 = num_papers1 / (years_active_1+1)\n",
    "    efficacy2 = num_papers2 / (years_active_2+1)\n",
    "    return efficacy1 + efficacy2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the ratio of venue to num. papers and keywords to num. papers\n",
    "def find_ratios(node1, node2):\n",
    "    total_venues1, total_keywords1 = total_venues_and_keywords(node1)\n",
    "    total_venues2, total_keywords2 = total_venues_and_keywords(node2)\n",
    "    num_papers1 = node1[\"num_papers\"] \n",
    "    num_papers2 = node2[\"num_papers\"] \n",
    "    \n",
    "    venue_ratio_1 = total_venues1 / num_papers1\n",
    "    venue_ratio_2 = total_venues2 / num_papers2\n",
    "    \n",
    "    keyword_ratio_1 = total_keywords1 / num_papers1\n",
    "    keyword_ratio_2 = total_keywords2 / num_papers2\n",
    "    \n",
    "    return venue_ratio_1 + venue_ratio_2, keyword_ratio_1 + keyword_ratio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 11, 7, 1, 10, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# computes the similarity of the node features from features.json\n",
    "# feature vector contains [combo degree, last paper, years active, # of shared keywords, # of shared venues, # of papers]\n",
    "def calculate_node_similarity(graph, node1, node2):\n",
    "    activity_vector = extract_activity_data(graph, node1, node2)\n",
    "    max_triangle = max(nx.triangles(graph,node1) , nx.triangles(graph,node2))\n",
    "    min_coeff = min(nx.clustering(graph,node1) , nx.clustering(graph,node2))\n",
    "    \n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]    \n",
    "    shared_venues, shared_keywords = shared_venues_and_keywords(node1, node2)\n",
    "    efficacy = find_efficacy(node1,node2)    \n",
    "    \n",
    "    feature_vector = [shared_venues, shared_keywords, min_coeff, max_triangle, efficacy]\n",
    "    return activity_vector + feature_vector\n",
    "\n",
    "v = calculate_node_similarity(graph, 0, 87)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4085/4085 [00:10<00:00, 388.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# generating a Node2Vec model to learn the node embeddings\n",
    "node2vec = Node2Vec(graph, dimensions=16, walk_length=80, num_walks=10, workers=4, p = 5, q = 1)\n",
    "model = node2vec.fit(window=6, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all of the gem modules\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the laplacian eigenmaps algorithm\n",
    "le = LaplacianEigenmaps(d=24)\n",
    "le_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the local linear embedding algorithm\n",
    "lli = LocallyLinearEmbedding(d=2)\n",
    "lli_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD error (low rank): 0.415458\n"
     ]
    }
   ],
   "source": [
    "# generating a HOPE model to learn node embeddings\n",
    "hope = HOPE(d=64, beta = .002)\n",
    "hope_graph_embeddings, t = hope.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the similarities between each node in the graph through their embeddings\n",
    "# take the dot product of the two vectors\n",
    "# length of the vector = 1\n",
    "# orthogonal = 0\n",
    "scores = []\n",
    "def get_cosine_similarity(edge):\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    node1_vector = model.wv.get_vector(str(node1))\n",
    "    node2_vector = model.wv.get_vector(str(node2))\n",
    "    cos_sim = np.dot(node1_vector, node2_vector)/(np.linalg.norm(node1_vector)*np.linalg.norm(node2_vector))\n",
    "    probability = max(0.0,cos_sim)\n",
    "    return min(1.0,probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_edges(g):\n",
    "    n_edges = g.number_of_edges()\n",
    "    n_nodes = g.number_of_nodes()\n",
    "    non_edges = [e for e in nx.non_edges(g)]\n",
    "    nneg = int(n_edges * 1.5)\n",
    "    n_neighbors = [len(list(g.neighbors(v))) for v in list(g.nodes)]\n",
    "    n_non_edges = n_nodes - 1 - np.array(n_neighbors)\n",
    "    rnd = np.random.RandomState(seed=None)\n",
    "    rnd_inx = rnd.choice(len(non_edges), nneg, replace=False)\n",
    "    neg_edge_list = [non_edges[i] for i in rnd_inx]\n",
    "    print(\"Len before pruning: \" + str(len(neg_edge_list)))\n",
    "    \n",
    "    for edge in neg_edge_list:\n",
    "        if get_cosine_similarity(edge) > .12:\n",
    "            neg_edge_list.remove(edge)\n",
    "    print(\"Len after pruning: \" + str(len(neg_edge_list)))\n",
    "    return neg_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before pruning: 44910\n",
      "Len after pruning: 33607\n",
      "len of training pos edges is 29940 and negative edges is 33607\n",
      "Len before pruning: 1585\n",
      "Len after pruning: 1175\n",
      "len of validation pos edges is 1057 and negative edges is 1175\n"
     ]
    }
   ],
   "source": [
    "training_pos_edges = graph.edges()\n",
    "training_neg_edges = generate_negative_edges(graph)\n",
    "\n",
    "print(\"len of training pos edges is \" + str(len(training_pos_edges)) \\\n",
    "      + \" and negative edges is \" + str(len(training_neg_edges))) \n",
    "validation_pos_edges = validation_graph.edges()\n",
    "validation_neg_edges = generate_negative_edges(validation_graph)\n",
    "print(\"len of validation pos edges is \" + str(len(validation_pos_edges)) + \\\n",
    "      \" and negative edges is \" + str(len(validation_neg_edges))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_features(graph, edge_list):\n",
    "    predictions = []\n",
    "    predictions.append(nx.jaccard_coefficient(graph, edge_list))\n",
    "    predictions.append(nx.adamic_adar_index(graph, edge_list))\n",
    "\n",
    "    scores_local = {}\n",
    "    for prediction_type in predictions:\n",
    "        for u, v, p in prediction_type:\n",
    "            if (u,v) in scores_local:\n",
    "                data = scores_local[(u,v)]\n",
    "                data = data + [p]\n",
    "                scores_local[(u,v)] = data\n",
    "            else:\n",
    "                scores_local[(u,v)] = [p]\n",
    "\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        features_scores = calculate_node_similarity(graph,node1,node2)\n",
    "        data = scores_local[(node1,node2)]\n",
    "        data = data + features_scores\n",
    "        scores_local[(node1,node2)] = data\n",
    "\n",
    "    return scores_local\n",
    "\n",
    "    \n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, edge_tuples)\n",
    "test_embedded_edges = [x for x in features_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 63547\n",
      "Labels: 63547\n"
     ]
    }
   ],
   "source": [
    "pos_labels = np.ones(len(local_features_pos_train))\n",
    "neg_labels = np.zeros(len(local_features_neg_train))\n",
    "training_labels = np.concatenate([pos_labels, neg_labels])\n",
    "training_features = np.concatenate([local_features_pos_train, local_features_neg_train])\n",
    "print(\"Features: \" + str(len(training_features)))\n",
    "print(\"Labels: \" + str(len(training_labels)))\n",
    "\n",
    "pos_labels = np.ones(len(local_features_pos_validation))\n",
    "neg_labels = np.zeros(len(local_features_neg_validation))\n",
    "validation_labels = np.concatenate([pos_labels, neg_labels])\n",
    "validation_features = np.concatenate([local_features_pos_validation, local_features_neg_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.125       0.86685447 72.          0.20168067  0.          0.\n",
      " 21.         18.          3.          8.          0.25757576 17.        ]\n"
     ]
    }
   ],
   "source": [
    "print(training_features[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to a logistic regression model using the embedded egde data\n",
    "from sklearn import metrics, model_selection, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=0)\n",
    "logistic_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(64, 64), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=250, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "neural_net = MLPClassifier(hidden_layer_sizes=(64,64),max_iter=350, activation = 'logistic')\n",
    "neural_net.fit(training_features + validation_features, training_labels + validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9241400367511787\n"
     ]
    }
   ],
   "source": [
    "# predicting the classes using the neural net\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "test_preds = neural_net.predict_proba(test_embedded_edges)[:, 1]\n",
    "validation_preds = neural_net.predict_proba(validation_features)[:, 1]\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.98995661e-01  6.45852267e+00 -6.43635665e-04  1.07283734e+00\n",
      "  -4.88829987e-02 -2.77073584e-02 -2.18337398e-03 -1.95062283e-02\n",
      "   2.87594510e-01  2.12071536e-02 -6.45458095e-01 -6.56016389e-05]]\n",
      "0.9249893782864327\n"
     ]
    }
   ],
   "source": [
    "# predicting the classes using the logistic model\n",
    "\n",
    "test_preds = logistic_model.predict_proba(test_embedded_edges)[:, 1]\n",
    "validation_preds = logistic_model.predict_proba(validation_features)[:, 1]\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(logistic_model.coef_)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file written\n"
     ]
    }
   ],
   "source": [
    "# writing the prediction probs to the output file\n",
    "with open('output.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Id\", \"Predicted\"])\n",
    "    for i in range(0, len(test_preds)):\n",
    "        prob = test_preds[i]\n",
    "        wr.writerow([i+1,float(prob)])\n",
    "                     \n",
    "print(\"output file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
