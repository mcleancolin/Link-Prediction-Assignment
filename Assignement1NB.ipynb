{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv, random, json\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4 (v3.7.4:e09359112e, Jul  8 2019, 14:54:52) \\n[Clang 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data into a list\n",
    "with open('train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(' ', '\\t') for line in lines]\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "\n",
    "training_file = []\n",
    "for row in d:\n",
    "    training_file.append([ int(x) for x in row ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data into a list\n",
    "test_file = pd.read_csv('test-public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the features of the nodes\n",
    "with open('nodes.json', 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the test data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for i,j in test_file.iterrows():\n",
    "    node_list_1.append(j[1])\n",
    "    node_list_2.append(j[2])\n",
    "\n",
    "test_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n",
    "edge_tuples = []\n",
    "for index, row in test_edge_list.iterrows():\n",
    "    edge_tuples.append((row['node_1'],row['node_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the training data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for row in training_file:\n",
    "    for connection in range(0,len(row)):\n",
    "        if connection == 0:\n",
    "            continue\n",
    "        else:\n",
    "            node_list_1.append(row[0])\n",
    "            node_list_2.append(row[connection])\n",
    "\n",
    "training_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53872\n",
      "43097\n",
      "43097\n",
      "10775\n",
      "10775\n"
     ]
    }
   ],
   "source": [
    "# splitting up the list of positive edges in the graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.ones(len(training_edge_list))\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_edge_list, y, test_size=0.20)\n",
    "#X_train = training_edge_list\n",
    "#y_train = np.ones(len(training_edge_list))\n",
    "print(len(training_edge_list))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a graph from the list of edges in the training data\n",
    "def create_graph(edge_list):\n",
    "    graph_edges = nx.from_pandas_edgelist(edge_list, \"node_1\", \"node_2\")\n",
    "    graph = nx.path_graph(4085)\n",
    "    graph.add_edges_from(graph_edges.edges())\n",
    "    n = graph.number_of_nodes()\n",
    "    m = graph.number_of_edges()\n",
    "    print(\"Number of nodes :\", str(n))\n",
    "    print(\"Number of edges :\", str(m))\n",
    "    print(\"Number of connected components :\" + str(nx.number_connected_components(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 29905\n",
      "Number of connected components :1\n",
      "-------------------------------\n",
      "Validation graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 13762\n",
      "Number of connected components :1\n"
     ]
    }
   ],
   "source": [
    "print(\"Training graph description: \")\n",
    "graph = create_graph(X_train)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "print(\"Validation graph description: \")\n",
    "validation_graph = create_graph(X_test)\n",
    "validation_graph.remove_edges_from(graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features to the nodes\n",
    "def add_features(graph):\n",
    "    for entry in features:\n",
    "        id = entry['id']\n",
    "        for key in entry:\n",
    "            graph.nodes[id][key] = entry[key]\n",
    "add_features(graph)\n",
    "add_features(validation_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the similarity of the node features from features.json\n",
    "# feature vector contains [combo degree, last paper, years active, # of shared keywords, # of shared venues, # of papers]\n",
    "def calculate_node_similarity(graph, node1, node2):\n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]\n",
    "    feature_vector = {}\n",
    "    for attribute in node1:\n",
    "        if attribute in node2:\n",
    "            if attribute[0:7] == \"keyword\":\n",
    "                feature_vector[\"keyword\"] += 1\n",
    "            if attribute[0:5] == \"venue\":\n",
    "                feature_vector[\"venue\"] += 1\n",
    "            \n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# extracts data from the nodes such as first paper, last paper, and the difference between them\n",
    "def extract_activity_data(graph, node1, node2):\n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]\n",
    "    F1 = node1[\"first\"]  \n",
    "    F2 = node2[\"first\"]\n",
    "    L1 = node1[\"last\"]    \n",
    "    L2 = node2[\"last\"]\n",
    "    years_active_1 = L1 - F1\n",
    "    years_active_2 = L2 - F2\n",
    "    \n",
    "\n",
    "extract_activity_data(graph, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4085/4085 [00:09<00:00, 416.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# generating a Node2Vec model to learn the node embeddings\n",
    "node2vec = Node2Vec(graph, dimensions=12, walk_length=80, num_walks=10, workers=4, p = 5, q = 1)\n",
    "model = node2vec.fit(window=6, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the edge embeddings of the node2vec model\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "edges_kv = edges_embs.as_keyed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all of the gem modules\n",
    "from gem.embedding.gf       import GraphFactorization\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding\n",
    "from gem.embedding.node2vec import node2vec\n",
    "#from gem.embedding.sdne     import SDNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the laplacian eigenmaps algorithm\n",
    "le = LaplacianEigenmaps(d=24)\n",
    "le_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the local linear embedding algorithm\n",
    "lli = LocallyLinearEmbedding(d=2)\n",
    "lli_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD error (low rank): 0.415458\n"
     ]
    }
   ],
   "source": [
    "# generating a HOPE model to learn node embeddings\n",
    "hope = HOPE(d=64, beta = .002)\n",
    "hope_graph_embeddings, t = hope.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a GF model to learn node embeddings\n",
    "embedding = GraphFactorization(d=2, max_iter=100000, eta=1*10**-4, regu=1.0, data_set='author')\n",
    "#embedding.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a SDNE model to learn node embeddings\n",
    "import keras\n",
    "import theano\n",
    "sdne = SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500)\n",
    "#sdne_graph_embeddings, t = sdne.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the similarities between each node in the graph through their embeddings\n",
    "# take the dot product of the two vectors\n",
    "# length of the vector = 1\n",
    "# orthogonal = 0\n",
    "scores = []\n",
    "def get_cosine_similarity(edge):\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    node1_vector = model.wv.get_vector(str(node1))\n",
    "    node2_vector = model.wv.get_vector(str(node2))\n",
    "    cos_sim = np.dot(node1_vector, node2_vector)/(np.linalg.norm(node1_vector)*np.linalg.norm(node2_vector))\n",
    "    probability = max(0.0,cos_sim)\n",
    "    return min(1.0,probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_edges(g):\n",
    "    n_edges = g.number_of_edges()\n",
    "    n_nodes = g.number_of_nodes()\n",
    "    non_edges = [e for e in nx.non_edges(g)]\n",
    "    nneg = int(n_edges * 2)\n",
    "    print(nneg)\n",
    "    n_neighbors = [len(list(g.neighbors(v))) for v in list(g.nodes)]\n",
    "    n_non_edges = n_nodes - 1 - np.array(n_neighbors)\n",
    "    rnd = np.random.RandomState(seed=None)\n",
    "    rnd_inx = rnd.choice(len(non_edges), nneg, replace=False)\n",
    "    neg_edge_list = [non_edges[i] for i in rnd_inx]\n",
    "    print(\"Len before pruning: \" + str(len(neg_edge_list)))\n",
    "    \n",
    "    for edge in neg_edge_list:\n",
    "        if get_cosine_similarity(edge) > .20:\n",
    "            neg_edge_list.remove(edge)\n",
    "    print(\"Len after pruning: \" + str(len(neg_edge_list)))\n",
    "    return neg_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59810\n",
      "Len before pruning: 59810\n",
      "Len after pruning: 46932\n",
      "len of training pos edges is 29905 and negative edges is 46932\n",
      "2184\n",
      "Len before pruning: 2184\n",
      "Len after pruning: 1730\n",
      "len of validation pos edges is 1092 and negative edges is 1730\n"
     ]
    }
   ],
   "source": [
    "training_pos_edges = graph.edges()\n",
    "training_neg_edges = generate_negative_edges(graph)\n",
    "\n",
    "print(\"len of training pos edges is \" + str(len(training_pos_edges)) \\\n",
    "      + \" and negative edges is \" + str(len(training_neg_edges))) \n",
    "validation_pos_edges = validation_graph.edges()\n",
    "validation_neg_edges = generate_negative_edges(validation_graph)\n",
    "print(\"len of validation pos edges is \" + str(len(validation_pos_edges)) + \\\n",
    "      \" and negative edges is \" + str(len(validation_neg_edges))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the scores using the jaccard_coefficient\n",
    "def make_local_features(graph, edge_list):\n",
    "    predictions = []\n",
    "    predictions.append(nx.jaccard_coefficient(graph, edge_list))\n",
    "    predictions.append(nx.adamic_adar_index(graph, edge_list))\n",
    "    predictions.append(nx.preferential_attachment(graph, edge_list))\n",
    "    predictions.append(nx.resource_allocation_index(graph, edge_list))\n",
    "\n",
    "    scores_local = {}\n",
    "    for prediction_type in predictions:\n",
    "        for u, v, p in prediction_type:\n",
    "            if (u,v) in scores_local:\n",
    "                data = list(scores_local.get((u,v), []))\n",
    "                data = [*data, p]\n",
    "                scores_local[(u,v)] = data\n",
    "            else:\n",
    "                scores_local[(u,v)] = [p]\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        node1_degree = graph.degree[node1]\n",
    "        node2_degree = graph.degree[node2]\n",
    "        total_degree = node1_degree + node2_degree\n",
    "        fatures_score = calculate_node_similarity(node1, node2)\n",
    "        \n",
    "        data = list(scores_local.get((node1,node2), []))\n",
    "        data = [*data, total_degree, fatures_score]\n",
    "        scores_local[(node1,node2)] = data\n",
    "        \n",
    "                \n",
    "    return scores_local\n",
    "    \n",
    "        \n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, edge_tuples)\n",
    "test_embedded_edges = [x for x in features_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 108511\n",
      "Labels: 108511\n"
     ]
    }
   ],
   "source": [
    "pos_labels = np.ones(len(local_features_pos_train))\n",
    "neg_labels = np.zeros(len(local_features_neg_train))\n",
    "training_labels = np.concatenate([pos_labels, neg_labels])\n",
    "training_features = np.concatenate([local_features_pos_train, local_features_neg_train])\n",
    "print(\"Features: \" + str(len(training_features)))\n",
    "print(\"Labels: \" + str(len(training_labels)))\n",
    "\n",
    "pos_labels = np.ones(len(local_features_pos_validation))\n",
    "neg_labels = np.zeros(len(local_features_neg_validation))\n",
    "validation_labels = np.concatenate([pos_labels, neg_labels])\n",
    "validation_features = np.concatenate([local_features_pos_validation, local_features_neg_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of the embedded node features for the training graph\n",
    "embedded_nodes = []\n",
    "for i in range(0,graph.number_of_nodes()):\n",
    "    node_embedding = model.wv.get_vector(str(i))\n",
    "    #node_embedding = hope_graph_embeddings[i]\n",
    "    embedded_nodes.append(node_embedding)\n",
    "embedded_matrix = np.vstack(embedded_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of embeddings (Hadmard Product of the two nodes) for the given edges\n",
    "def get_edge_embeddings(edge_list):\n",
    "    embedded_edges = []\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        node1_embedded = embedded_matrix[node1]\n",
    "        node2_embedded = embedded_matrix[node2]\n",
    "        edge_embedded = np.multiply(node1_embedded, node2_embedded)\n",
    "        embedded_edges.append(edge_embedded)\n",
    "    embedded_edges = np.array(embedded_edges)\n",
    "    return embedded_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78855\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedded list of training features\n",
    "positive_embedded_edges = get_edge_embeddings(training_pos_edges)\n",
    "negative_embedded_edges = get_edge_embeddings(training_neg_edges)\n",
    "training_features = np.concatenate([positive_embedded_edges, negative_embedded_edges])\n",
    "print(len(training_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78855\n"
     ]
    }
   ],
   "source": [
    "# Creating the label vector to train the data\n",
    "listofones = [1] * len(positive_embedded_edges)\n",
    "listofzeros = [0] * len(negative_embedded_edges)\n",
    "training_labels = listofones + listofzeros\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the embedded list of test and validation features\n",
    "test_embedded_edges = get_edge_embeddings(edge_tuples)\n",
    "validation_pos_embedded_edges = get_edge_embeddings(validation_pos_edges)\n",
    "validation_neg_embedded_edges = get_edge_embeddings(validation_neg_edges)\n",
    "validation_features = np.concatenate([validation_pos_embedded_edges, validation_neg_embedded_edges])\n",
    "listofones = [1] * len(validation_pos_edges)\n",
    "listofzeros = [0] * len(validation_neg_edges)\n",
    "validation_labels = listofones + listofzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to a logistic regression model using the embedded egde data\n",
    "from sklearn import metrics, model_selection, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=0)\n",
    "logistic_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=70,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to a random forest model using the embedded egde data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_model = RandomForestClassifier(n_estimators=70, bootstrap = True,max_features = 'sqrt')\n",
    "random_forest_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(8, 8, 8, 8), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=400, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "neural_net = MLPClassifier(hidden_layer_sizes=(8,8,8,8),max_iter=400, activation = 'logistic')\n",
    "neural_net.fit(training_features,training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87672083 0.93356577 0.00276022 ... 0.04733783 0.97868531 0.00294644]\n",
      "[0.62240186 0.95107441 0.99462699 ... 0.00338865 0.00282997 0.00261909]\n",
      "0.9644288279731074\n"
     ]
    }
   ],
   "source": [
    "test_preds = neural_net.predict_proba(test_embedded_edges)[:, 1]\n",
    "print(test_preds)\n",
    "validation_preds = neural_net.predict_proba(validation_features)[:, 1]\n",
    "print(validation_preds)\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.86742629e-01 9.28613170e-01 2.93056902e-03 ... 2.04666761e-01\n",
      " 9.91889841e-01 1.30297519e-04]\n",
      "[0.20422338 0.98074781 0.9999817  ... 0.00150234 0.00683831 0.00121563]\n",
      "0.9628482168034551\n"
     ]
    }
   ],
   "source": [
    "# predicting the classes\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "test_preds = logistic_model.predict_proba(test_embedded_edges)[:, 1]\n",
    "print(test_preds)\n",
    "validation_preds = logistic_model.predict_proba(validation_features)[:, 1]\n",
    "print(validation_preds)\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52857143 0.62857143 0.         ... 0.21428571 0.75714286 0.01428571]\n",
      "[0.35714286 0.74285714 0.72857143 ... 0.02857143 0.         0.        ]\n",
      "0.9588883405808931\n"
     ]
    }
   ],
   "source": [
    "test_preds = random_forest_model.predict_proba(test_embedded_edges)[:, 1]\n",
    "print(test_preds)\n",
    "validation_preds = random_forest_model.predict_proba(validation_features)[:, 1]\n",
    "print(validation_preds)\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file written\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Id\", \"Predicted\"])\n",
    "    for i in range(0, len(test_preds)):\n",
    "        prob = test_preds[i]\n",
    "        wr.writerow([i+1,float(prob)])\n",
    "                     \n",
    "print(\"output file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
