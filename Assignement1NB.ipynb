{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv, random, json\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4 (v3.7.4:e09359112e, Jul  8 2019, 14:54:52) \\n[Clang 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!utils/python/3.6.0\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data into a list\n",
    "with open('train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(' ', '\\t') for line in lines]\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "\n",
    "training_file = []\n",
    "for row in d:\n",
    "    training_file.append([ int(x) for x in row ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data into a list\n",
    "test_file = pd.read_csv('test-public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the features of the nodes\n",
    "with open('nodes.json', 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the test data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for i,j in test_file.iterrows():\n",
    "    node_list_1.append(j[1])\n",
    "    node_list_2.append(j[2])\n",
    "\n",
    "test_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n",
    "edge_tuples = []\n",
    "for index, row in test_edge_list.iterrows():\n",
    "    edge_tuples.append((row['node_1'],row['node_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the training data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for row in training_file:\n",
    "    for connection in range(0,len(row)):\n",
    "        if connection == 0:\n",
    "            continue\n",
    "        else:\n",
    "            node_list_1.append(row[0])\n",
    "            node_list_2.append(row[connection])\n",
    "\n",
    "training_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53872\n",
      "45791\n",
      "45791\n",
      "8081\n",
      "8081\n"
     ]
    }
   ],
   "source": [
    "# splitting up the list of positive edges in the graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.ones(len(training_edge_list))\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_edge_list, y, test_size=0.15)\n",
    "#X_train = training_edge_list\n",
    "#y_train = np.ones(len(training_edge_list))\n",
    "print(len(training_edge_list))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a graph from the list of edges in the training data\n",
    "def create_graph(edge_list):\n",
    "    graph_edges = nx.from_pandas_edgelist(edge_list, \"node_1\", \"node_2\")\n",
    "    graph = nx.path_graph(4085)\n",
    "    graph.add_edges_from(graph_edges.edges())\n",
    "    n = graph.number_of_nodes()\n",
    "    m = graph.number_of_edges()\n",
    "    print(\"Number of nodes :\", str(n))\n",
    "    print(\"Number of edges :\", str(m))\n",
    "    print(\"Number of connected components :\" + str(nx.number_connected_components(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 30407\n",
      "Number of connected components :1\n",
      "-------------------------------\n",
      "Validation graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 11566\n",
      "Number of connected components :1\n"
     ]
    }
   ],
   "source": [
    "print(\"Training graph description: \")\n",
    "graph = create_graph(X_train)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "print(\"Validation graph description: \")\n",
    "validation_graph = create_graph(X_test)\n",
    "validation_graph.remove_edges_from(graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features to the nodes\n",
    "def add_features(graph):\n",
    "    for entry in features:\n",
    "        id = entry['id']\n",
    "        for key in entry:\n",
    "            graph.nodes[id][key] = entry[key]\n",
    "add_features(graph)\n",
    "add_features(validation_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 11, 5, 10, 1, 1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# computes the similarity of the node features from features.json\n",
    "# feature vector contains [combo degree, last paper, years active, # of shared keywords, # of shared venues, # of papers]\n",
    "def calculate_node_similarity(graph, node1, node2):\n",
    "    activity_vector = extract_activity_data(graph, node1, node2)\n",
    "    max_triangle = max(nx.triangles(graph,node1) , nx.triangles(graph,node2))\n",
    "    #min_triangle = min(nx.triangles(graph,node1) , nx.triangles(graph,node2))\n",
    "    max_coeff = min(nx.clustering(graph,node1) , nx.clustering(graph,node2))\n",
    "    #path_len = nx.shortest_path_length(graph, source=node1, target=node2)\n",
    "    \n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]\n",
    "    feature_dict = {\"keyword\":0, \"venue\":0}\n",
    "    for attribute in node1:\n",
    "        if attribute in node2:\n",
    "            if attribute[0:7] == \"keyword\":\n",
    "                feature_dict[\"keyword\"] += 1\n",
    "            if attribute[0:5] == \"venue\":\n",
    "                feature_dict[\"venue\"] += 1\n",
    "    feature_vector = [feature_dict[\"keyword\"], feature_dict[\"venue\"], \\\n",
    "                      max_triangle, max_coeff]\n",
    "    return activity_vector + feature_vector\n",
    "\n",
    "v = calculate_node_similarity(graph, 0, 87)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts data from the nodes such as first paper, last paper, and the difference between them\n",
    "def extract_activity_data(graph, node1, node2):\n",
    "    node1 = graph.nodes[node1]\n",
    "    node2 = graph.nodes[node2]\n",
    "    F1 = node1[\"first\"]  \n",
    "    F2 = node2[\"first\"]\n",
    "    L1 = node1[\"last\"]    \n",
    "    L2 = node2[\"last\"]\n",
    "    num_papers1 = node1[\"num_papers\"] \n",
    "    num_papers2 = node2[\"num_papers\"] \n",
    "    years_active_1 = F1 - L1\n",
    "    years_active_2 = F2 - L2\n",
    "    vector = [abs(F1-F2), abs(L1-L2), num_papers1+num_papers2, years_active_1+years_active_2]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4085/4085 [00:11<00:00, 352.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# generating a Node2Vec model to learn the node embeddings\n",
    "node2vec = Node2Vec(graph, dimensions=32, walk_length=80, num_walks=10, workers=4, p = 6, q = 1)\n",
    "model = node2vec.fit(window=6, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all of the gem modules\n",
    "from gem.embedding.gf       import GraphFactorization\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding\n",
    "from gem.embedding.node2vec import node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gem.embedding.sdne import SDNE\n",
    "sdne = SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3, n_units=[50, 15,], n_iter=50, xeta=0.01, n_batch=500,\n",
    "                modelfile=['enc_model.json', 'dec_model.json'],\n",
    "                weightfile=['enc_weights.hdf5', 'dec_weights.hdf5'])\n",
    "#sdne.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the laplacian eigenmaps algorithm\n",
    "le = LaplacianEigenmaps(d=24)\n",
    "le_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix recon. error (low rank): 66.740138\n"
     ]
    }
   ],
   "source": [
    "# learning the embeddings uing the local linear embedding algorithm\n",
    "lli = LocallyLinearEmbedding(d=2)\n",
    "lli_embeddings, t = le.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD error (low rank): 0.415458\n"
     ]
    }
   ],
   "source": [
    "# generating a HOPE model to learn node embeddings\n",
    "hope = HOPE(d=64, beta = .002)\n",
    "hope_graph_embeddings, t = hope.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the similarities between each node in the graph through their embeddings\n",
    "# take the dot product of the two vectors\n",
    "# length of the vector = 1\n",
    "# orthogonal = 0\n",
    "scores = []\n",
    "def get_cosine_similarity(edge):\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    node1_vector = model.wv.get_vector(str(node1))\n",
    "    node2_vector = model.wv.get_vector(str(node2))\n",
    "    cos_sim = np.dot(node1_vector, node2_vector)/(np.linalg.norm(node1_vector)*np.linalg.norm(node2_vector))\n",
    "    probability = max(0.0,cos_sim)\n",
    "    return min(1.0,probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_edges(g):\n",
    "    n_edges = g.number_of_edges()\n",
    "    n_nodes = g.number_of_nodes()\n",
    "    non_edges = [e for e in nx.non_edges(g)]\n",
    "    nneg = int(n_edges * 1.5)\n",
    "    n_neighbors = [len(list(g.neighbors(v))) for v in list(g.nodes)]\n",
    "    n_non_edges = n_nodes - 1 - np.array(n_neighbors)\n",
    "    rnd = np.random.RandomState(seed=None)\n",
    "    rnd_inx = rnd.choice(len(non_edges), nneg, replace=False)\n",
    "    neg_edge_list = [non_edges[i] for i in rnd_inx]\n",
    "    print(\"Len before pruning: \" + str(len(neg_edge_list)))\n",
    "    \n",
    "    for edge in neg_edge_list:\n",
    "        if get_cosine_similarity(edge) > .12:\n",
    "            neg_edge_list.remove(edge)\n",
    "    print(\"Len after pruning: \" + str(len(neg_edge_list)))\n",
    "    return neg_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len before pruning: 45610\n",
      "Len after pruning: 36035\n",
      "len of training pos edges is 30407 and negative edges is 36035\n",
      "Len before pruning: 885\n",
      "Len after pruning: 677\n",
      "len of validation pos edges is 590 and negative edges is 677\n"
     ]
    }
   ],
   "source": [
    "training_pos_edges = graph.edges()\n",
    "training_neg_edges = generate_negative_edges(graph)\n",
    "\n",
    "print(\"len of training pos edges is \" + str(len(training_pos_edges)) \\\n",
    "      + \" and negative edges is \" + str(len(training_neg_edges))) \n",
    "validation_pos_edges = validation_graph.edges()\n",
    "validation_neg_edges = generate_negative_edges(validation_graph)\n",
    "print(\"len of validation pos edges is \" + str(len(validation_pos_edges)) + \\\n",
    "      \" and negative edges is \" + str(len(validation_neg_edges))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_features(graph, edge_list):\n",
    "    predictions = []\n",
    "    predictions.append(nx.jaccard_coefficient(graph, edge_list))\n",
    "    predictions.append(nx.adamic_adar_index(graph, edge_list))\n",
    "    predictions.append(nx.preferential_attachment(graph, edge_list))\n",
    "    predictions.append(nx.resource_allocation_index(graph, edge_list))\n",
    "\n",
    "    scores_local = {}\n",
    "    for prediction_type in predictions:\n",
    "        for u, v, p in prediction_type:\n",
    "            if (u,v) in scores_local:\n",
    "                data = scores_local[(u,v)]\n",
    "                data = data + [p]\n",
    "                scores_local[(u,v)] = data\n",
    "            else:\n",
    "                scores_local[(u,v)] = [p]\n",
    "\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        features_scores = calculate_node_similarity(graph,node1,node2)\n",
    "        data = scores_local[(node1,node2)]\n",
    "        data = data + features_scores\n",
    "        scores_local[(node1,node2)] = data\n",
    "\n",
    "    return scores_local\n",
    "\n",
    "    \n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_train = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_pos_edges)\n",
    "local_features_pos_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, training_neg_edges)\n",
    "local_features_neg_validation = [x for x in features_dict.values()]\n",
    "\n",
    "features_dict = make_local_features(graph, edge_tuples)\n",
    "test_embedded_edges = [x for x in features_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 66442\n",
      "Labels: 66442\n"
     ]
    }
   ],
   "source": [
    "pos_labels = np.ones(len(local_features_pos_train))\n",
    "neg_labels = np.zeros(len(local_features_neg_train))\n",
    "training_labels = np.concatenate([pos_labels, neg_labels])\n",
    "training_features = np.concatenate([local_features_pos_train, local_features_neg_train])\n",
    "print(\"Features: \" + str(len(training_features)))\n",
    "print(\"Labels: \" + str(len(training_labels)))\n",
    "\n",
    "pos_labels = np.ones(len(local_features_pos_validation))\n",
    "neg_labels = np.zeros(len(local_features_neg_validation))\n",
    "validation_labels = np.concatenate([pos_labels, neg_labels])\n",
    "validation_features = np.concatenate([local_features_pos_validation, local_features_neg_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.97029703e-02 8.72964481e-01 2.17500000e+03 1.07870370e-01\n",
      " 0.00000000e+00 0.00000000e+00 1.61000000e+02 1.80000000e+01\n",
      " 2.40000000e+01 6.00000000e+00 1.94000000e+02 6.99099099e-02]\n"
     ]
    }
   ],
   "source": [
    "print(training_features[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of the embedded node features for the training graph\n",
    "embedded_nodes = []\n",
    "for i in range(0,graph.number_of_nodes()):\n",
    "    node_embedding = model.wv.get_vector(str(i))\n",
    "    #node_embedding = hope_graph_embeddings[i]\n",
    "    embedded_nodes.append(node_embedding)\n",
    "embedded_matrix = np.vstack(embedded_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of embeddings (Hadmard Product of the two nodes) for the given edges\n",
    "def get_edge_embeddings(edge_list):\n",
    "    embedded_edges = []\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        node1_embedded = embedded_matrix[node1]\n",
    "        node2_embedded = embedded_matrix[node2]\n",
    "        edge_embedded = np.multiply(node1_embedded, node2_embedded)\n",
    "        feature_vector = calculate_node_similarity(graph, node1, node2)\n",
    "        edge_embedded = np.append(edge_embedded, feature_vector)\n",
    "        embedded_edges.append(edge_embedded)\n",
    "    embedded_edges = np.array(embedded_edges)\n",
    "    return embedded_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90647\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedded list of training features\n",
    "positive_embedded_edges = get_edge_embeddings(training_pos_edges)\n",
    "negative_embedded_edges = get_edge_embeddings(training_neg_edges)\n",
    "training_features = np.concatenate([positive_embedded_edges, negative_embedded_edges])\n",
    "print(len(training_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90647\n"
     ]
    }
   ],
   "source": [
    "# Creating the label vector to train the data\n",
    "listofones = [1] * len(positive_embedded_edges)\n",
    "listofzeros = [0] * len(negative_embedded_edges)\n",
    "training_labels = listofones + listofzeros\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the embedded list of test and validation features\n",
    "test_embedded_edges = get_edge_embeddings(edge_tuples)\n",
    "validation_pos_embedded_edges = get_edge_embeddings(validation_pos_edges)\n",
    "validation_neg_embedded_edges = get_edge_embeddings(validation_neg_edges)\n",
    "validation_features = np.concatenate([validation_pos_embedded_edges, validation_neg_embedded_edges])\n",
    "listofones = [1] * len(validation_pos_edges)\n",
    "listofzeros = [0] * len(validation_neg_edges)\n",
    "validation_labels = listofones + listofzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to a logistic regression model using the embedded egde data\n",
    "from sklearn import metrics, model_selection, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=0)\n",
    "logistic_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "neural_net = MLPClassifier(hidden_layer_sizes=(64,64),max_iter=400, activation = 'logistic')\n",
    "neural_net.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_preds = neural_net.predict_proba(test_embedded_edges)[:, 1]\n",
    "validation_preds = neural_net.predict_proba(validation_features)[:, 1]\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.73040364e-01  6.94696931e+00 -8.35342065e-04  1.16791447e+00\n",
      "  -2.30319710e-02 -1.62001751e-02 -3.12967957e-03 -1.32545183e-03\n",
      "   2.76396628e-02  2.79663424e-01 -7.48700198e-05 -8.16523195e-01]]\n",
      "0.9258921857090838\n"
     ]
    }
   ],
   "source": [
    "# predicting the classes\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "test_preds = logistic_model.predict_proba(test_embedded_edges)[:, 1]\n",
    "validation_preds = logistic_model.predict_proba(validation_features)[:, 1]\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(logistic_model.coef_)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file written\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Id\", \"Predicted\"])\n",
    "    for i in range(0, len(test_preds)):\n",
    "        prob = test_preds[i]\n",
    "        wr.writerow([i+1,float(prob)])\n",
    "                     \n",
    "print(\"output file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
