{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv, random, json\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data into a list\n",
    "with open('train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = [line.replace(' ', '\\t') for line in lines]\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\n",
    "\n",
    "training_file = []\n",
    "for row in d:\n",
    "    training_file.append([ int(x) for x in row ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data into a list\n",
    "test_file = pd.read_csv('test-public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the features of the nodes\n",
    "with open('nodes.json', 'r') as f:\n",
    "    features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the test data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for i,j in test_file.iterrows():\n",
    "    node_list_1.append(j[1])\n",
    "    node_list_2.append(j[2])\n",
    "\n",
    "test_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n",
    "edge_tuples = []\n",
    "for index, row in test_edge_list.iterrows():\n",
    "    edge_tuples.append((row['node_1'],row['node_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the training data into a pandas dataframe list of edges\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for row in training_file:\n",
    "    for connection in range(0,len(row)):\n",
    "        if connection == 0:\n",
    "            continue\n",
    "        else:\n",
    "            node_list_1.append(row[0])\n",
    "            node_list_2.append(row[connection])\n",
    "\n",
    "training_edge_list = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53872\n",
      "37710\n",
      "37710\n",
      "16162\n",
      "16162\n"
     ]
    }
   ],
   "source": [
    "# splitting up the list of positive edges in the graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.ones(len(training_edge_list))\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_edge_list, y, test_size=0.3)\n",
    "print(len(training_edge_list))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a graph from the list of edges in the training data\n",
    "def create_graph(edge_list):\n",
    "    graph_edges = nx.from_pandas_edgelist(edge_list, \"node_1\", \"node_2\")\n",
    "    graph = nx.path_graph(4085)\n",
    "    graph.add_edges_from(graph_edges.edges())\n",
    "    n = graph.number_of_nodes()\n",
    "    m = graph.number_of_edges()\n",
    "    print(\"Number of nodes :\", str(n))\n",
    "    print(\"Number of edges :\", str(m))\n",
    "    print(\"Number of connected components :\" + str(nx.number_connected_components(graph)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 28609\n",
      "Number of connected components :1\n",
      "-------------------------------\n",
      "Validation graph description: \n",
      "Number of nodes : 4085\n",
      "Number of edges : 17846\n",
      "Number of connected components :1\n"
     ]
    }
   ],
   "source": [
    "print(\"Training graph description: \")\n",
    "graph = create_graph(X_train)\n",
    "print(\"-------------------------------\")\n",
    "print(\"Validation graph description: \")\n",
    "validation_graph = create_graph(X_test)\n",
    "validation_graph.remove_edges_from(graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features to the nodes\n",
    "def add_features(graph):\n",
    "    for entry in features:\n",
    "        id = entry['id']\n",
    "        for key in entry:\n",
    "            graph.nodes[id][key] = entry[key]\n",
    "add_features(graph)\n",
    "add_features(validation_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions the graph into communities of nodes\n",
    "from community import community_louvain\n",
    "community_dict = community_louvain.best_partition(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores/labels calculated\n"
     ]
    }
   ],
   "source": [
    "# predicting the scores using the jaccard_coefficient\n",
    "predictions_jac = nx.jaccard_coefficient(graph, edge_tuples)\n",
    "\n",
    "scores_jac = []\n",
    "for u, v, p in predictions_jac:\n",
    "    scores_jac.append([u,v,p])\n",
    "\n",
    "print(\"scores/labels calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 4085/4085 [00:09<00:00, 437.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# generating a Node2Vec model to learn the node embeddings\n",
    "node2vec = Node2Vec(graph, dimensions=96, walk_length=100, num_walks=10, workers=4, p = 7, q = 2)\n",
    "model = node2vec.fit(window=6, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating edge features: 100%|██████████| 8345655/8345655.0 [01:02<00:00, 132931.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# generating the edge embeddings of the node2vec model\n",
    "edges_kv = edges_embs.as_keyed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD error (low rank): 0.003905\n"
     ]
    }
   ],
   "source": [
    "from gem.embedding.hope import HOPE\n",
    "hope = HOPE(d=128, beta = .00002)\n",
    "hope_graph_embeddings, t = hope.learn_embedding(graph=graph, edge_f=None, is_weighted=False, no_python=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the similarities between each node in the graph through their embeddings\n",
    "# take the dot product of the two vectors\n",
    "# length of the vector = 1\n",
    "# orthogonal = 0\n",
    "scores = []\n",
    "for edge in edge_tuples:\n",
    "    node1 = edge[0]\n",
    "    node2 = edge[1]\n",
    "    #print(node1, node2)\n",
    "    node1_vector = model.wv.get_vector(str(node1))\n",
    "    node2_vector = model.wv.get_vector(str(node2))\n",
    "    cos_sim = np.dot(node1_vector, node2_vector)/(np.linalg.norm(node1_vector)*np.linalg.norm(node2_vector))\n",
    "    probability = max(0.0,cos_sim)\n",
    "    scores.append(min(1.0,probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file written\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Id\", \"Predicted\"])\n",
    "    for i in range(0, len(scores)):\n",
    "        wr.writerow([i+1,scores[i]])\n",
    "                     \n",
    "print(\"output file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_edges(graph):\n",
    "    n_edges = graph.number_of_edges()\n",
    "    n_nodes = graph.number_of_nodes()\n",
    "    nneg = int(n_edges)\n",
    "    n_neighbors = [len(list(graph.neighbors(v))) for v in list(graph.nodes)]\n",
    "    n_non_edges = n_nodes - 1 - np.array(n_neighbors)\n",
    "    non_edges = [e for e in nx.non_edges(graph)]\n",
    "    rnd = np.random.RandomState(seed=None)\n",
    "    rnd_inx = rnd.choice(len(non_edges), nneg, replace=False)\n",
    "    neg_edge_list = [non_edges[i] for i in rnd_inx]\n",
    "    return neg_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of training pos edges is 28609 and negative edges is 28609\n",
      "len of validation pos edges is 2388 and negative edges is 2388\n"
     ]
    }
   ],
   "source": [
    "training_pos_edges = graph.edges()\n",
    "training_neg_edges = generate_negative_edges(graph)\n",
    "print(\"len of training pos edges is \" + str(len(training_pos_edges)) \\\n",
    "      + \" and negative edges is \" + str(len(training_neg_edges))) \n",
    "validation_pos_edges = validation_graph.edges()\n",
    "validation_neg_edges = generate_negative_edges(validation_graph)\n",
    "print(\"len of validation pos edges is \" + str(len(validation_pos_edges)) + \\\n",
    "      \" and negative edges is \" + str(len(validation_neg_edges))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of the embedded node features for the training graph\n",
    "embedded_nodes = []\n",
    "for i in range(0,graph.number_of_nodes()):\n",
    "    node_embedding = model.wv.get_vector(str(i))\n",
    "    embedded_nodes.append(node_embedding)\n",
    "embedded_matrix = np.vstack(embedded_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of embeddings (Hadmard Product of the two nodes) for the given edges\n",
    "def get_edge_embeddings(edge_list):\n",
    "    embedded_edges = []\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        node1_embedded = embedded_matrix[node1]\n",
    "        node2_embedded = embedded_matrix[node2]\n",
    "        edge_embedded = np.multiply(node1_embedded, node2_embedded)\n",
    "        embedded_edges.append(edge_embedded)\n",
    "    embedded_edges = np.array(embedded_edges)\n",
    "    return embedded_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedded list of training features\n",
    "positive_embedded_edges = get_edge_embeddings(pos_edge_list)\n",
    "negative_embedded_edges = get_edge_embeddings(neg_edge_list)\n",
    "training_features = np.concatenate([positive_embedded_edges, negative_embedded_edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30279\n"
     ]
    }
   ],
   "source": [
    "# Creating the label vector to train the data\n",
    "listofones = [1] * len(pos_edge_list)\n",
    "listofzeros = [0] * len(neg_edge_list)\n",
    "training_labels = listofones + listofzeros\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the embedded list of test and validation features\n",
    "test_embedded_edges = get_edge_embeddings(edge_tuples)\n",
    "validation_pos_embedded_edges = get_edge_embeddings(validation_pos_edges)\n",
    "validation_neg_embedded_edges = get_edge_embeddings(validation_neg_edges)\n",
    "validation_features = np.concatenate([validation_pos_embedded_edges, validation_neg_embedded_edges])\n",
    "listofones = [1] * len(validation_pos_edges)\n",
    "listofzeros = [0] * len(validation_neg_edges)\n",
    "validation_labels = listofones + listofzeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to a logistic regression model using the embedded egde data\n",
    "from sklearn import metrics, model_selection, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=0)\n",
    "logistic_model.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.59808517e-01 4.20194443e-01 3.45666792e-04 ... 2.78864352e-02\n",
      " 7.65025814e-01 4.43071768e-06]\n",
      "[9.99797199e-01 9.99373078e-01 9.98438272e-01 ... 1.08951036e-04\n",
      " 2.99701609e-05 7.17282451e-03]\n",
      "0.957123522413856\n"
     ]
    }
   ],
   "source": [
    "# predicting the classes\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "test_preds = logistic_model.predict_proba(test_embedded_edges)[:, 1]\n",
    "print(test_preds)\n",
    "validation_preds = logistic_model.predict_proba(validation_features)[:, 1]\n",
    "print(validation_preds)\n",
    "auc = roc_auc_score(validation_labels, validation_preds)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file written\n"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w', newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Id\", \"Predicted\"])\n",
    "    for i in range(0, len(scores)):\n",
    "        prob = test_preds[i]\n",
    "        wr.writerow([i+1,float(prob)])\n",
    "                     \n",
    "print(\"output file written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
